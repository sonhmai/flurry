# Guard and Memory Management

<!-- TOC -->
* [Guard and Memory Management](#guard-and-memory-management)
  * [Overview](#overview)
  * [Why need Guard?](#why-need-guard)
  * [How Guard works?](#how-guard-works)
    * [Cleaning up in runtime or compile-time?](#cleaning-up-in-runtime-or-compile-time)
  * [Alternatives](#alternatives)
    * [Arc](#arc)
<!-- TOC -->

## Overview
Context:
- Operations that access or modify data require a Guard to ensure memory safety.
- Flurry uses a guard-based memory management system provided by `seize` crate to ensure safe concurrent access.

```rust
let map: HashMap<i32, String> = HashMap::new();
// pin the current epoch
let guard = map.guard();

map.insert(1, "one".to_string(), &guard);
let value = map.get(&1, &guard);
map.remove(&1, &guard);
// when guard drops, thread is unpinned
```

```rust
use flurry::HashMap;

let map = HashMap::new();

// Writer thread
map.insert(42, "hello");

// Reader thread
let guard = map.guard(); // Pin epoch
if let Some(value) = map.get(&42, &guard) {
    // Safe to use value here
    println!("{}", value);
} 
// guard drops, unpinning epoch

// NEVER do this:
// let guard = map.guard();
// let value = map.get(&42, &guard);
// drop(guard); // Drops too early!
// println!("{}", value); // Use-after-free!
```

## Why need Guard?
- Java's ConcurrentHashMap relies on Java's GC, but Rust doesn't have runtime GC.
- it's a memory safety mechanism.
- it prevents **premature deallocation** of shared data in concurrent operations.

## How Guard works?

When values are removed from the map, they go through a retirement process rather than immediate deallocation.
- Each thread pins itself (creates a Guard) before touching the map. 
- Readers can load a value and use it for the lifetime of that guard. 
- Writers retire removed values via retire_shared; real freeing is delayed until all guards from the epoch in which the value was retired have been dropped.

```mermaid
sequenceDiagram
    participant T1 as "Thread 1"
    participant T2 as "Thread 2"
    participant HM as HashMap
    participant GC as "seize::Collector"
    participant MEM as Memory

    %% --- Thread 1 reads ------------------------------
    T1->>HM: guard()
    HM->>GC: enter()
    GC-->>T1: Guard (marks thread active)
    T1->>HM: get(key,&guard)
    HM->>MEM: load value
    MEM-->>HM: value
    HM-->>T1: reference to value

    %% --- Thread 2 removes ----------------------------
    T2->>HM: guard()
    HM->>GC: enter()
    GC-->>T2: Guard (marks thread active)
    T2->>HM: remove(key,&guard)
    HM->>GC: retire_shared(old_value)
    Note over GC: Value marked for cleanup but not freed

    %% --- Guards dropped ------------------------------
    T1->>GC: drop guard
    Note over GC: Thread 1 no longer active
    
    T2->>GC: drop guard
    Note over GC: Thread 2 no longer active
    
    %% --- Reclamation --------------------------------
    GC->>MEM: Safe to reclaim retired values
```

### Cleaning up in runtime or compile-time?
- cleaning up or entries having no more reference happens at runtime.
- (using crossbeam-epoch), cleanup happens opportunistically in user-threads, there's no GC thread.
- `work stealing`: threads that create guards help clean up garbage from all threads.

Example
```rust
// Thread A
map.insert(1, "a");
map.remove(&1);  // Item goes to Thread A's garbage bag

// Thread B  
// Thread B might clean Thread A's garbage!
// IF conditions are met (all threads past epoch)
let guard = map.guard();  

// Thread C
map.insert(2, "b");  // Might also trigger cleanup of old garbage
```

Mechanism
```rust
// Simplified view of what happens inside crossbeam-epoch
// THRESHOLD 32 or 64 items
impl Local {
    fn pin(&self) -> Guard {
        // 1. Pin to current epoch
        let guard = self.inner.pin();
        
        // 2. Check if we should help with cleanup
        if self.garbage_count() > THRESHOLD {
            // 3. This thread tries to advance the global epoch
            self.try_advance_epoch();
            
            // 4. Collect garbage from old epochs
            self.collect_garbage();
        }
        
        guard
    }
}
```

## Testing

https://github.com/jonhoo/flurry/blob/c0a84ac093d73b69916ed873ca8a9119b08d177b/tests/basic.rs#L365-L381
```rust

```

## Alternatives
1. Runtime Garbage Collection - Java's ConcurrentHashMap relies on Java's GC, but Rust doesn't have runtime GC
2. Atomic Reference Counting - Would be less efficient than the batch reference-counting approach used by seize
3. Global Locks - Would eliminate the concurrency benefits that make concurrent hash maps valuable

| Approach | Pros | Cons | Best Use Case |
|----------|------|------|---------------|
| **Guards/EBR** (flurry) | âœ… Very low read overhead (just local counter)<br>âœ… No per-object memory overhead<br>âœ… Batched reclamation (efficient)<br>âœ… Natural Rust API with lifetimes<br>âœ… Excellent cache locality<br>âœ… Scales well with read-heavy workloads | âŒ Memory can accumulate if thread stays pinned<br>âŒ Can't hold guards across blocking ops<br>âŒ Coarse-grained (protects everything in epoch)<br>âŒ Requires careful API design<br>âŒ Unbounded memory in pathological cases | Lock-free data structures with frequent reads, short critical sections |
| **Reference Counting** (Arc) | âœ… Simple and familiar API<br>âœ… Immediate reclamation<br>âœ… Works with any data structure<br>âœ… Predictable memory usage<br>âœ… No special guards needed<br>âœ… Safe across async boundaries | âŒ High overhead (atomics on every clone/drop)<br>âŒ Cache line contention<br>âŒ Can't handle cyclic references<br>âŒ Poor scalability under contention<br>âŒ Extra indirection | Simple concurrent access, clear ownership patterns, async code |
| **Hazard Pointers** | âœ… Bounded memory usage<br>âœ… Fine-grained protection<br>âœ… Can protect specific pointers<br>âœ… Works with long operations<br>âœ… Predictable worst-case memory | âŒ Higher per-operation overhead<br>âŒ Complex API (manual protection)<br>âŒ Limited number of pointers<br>âŒ Requires careful pointer tracking<br>âŒ More bookkeeping | Long-running operations, systems requiring bounded memory |
| **RCU** | âœ… Extremely low read overhead<br>âœ… Wait-free reads<br>âœ… Good for read-dominated workloads<br>âœ… Proven in OS kernels<br>âœ… Allows complex read-side operations | âŒ Writers must wait for grace periods<br>âŒ Complex implementation<br>âŒ Not native to Rust ecosystem<br>âŒ Can have high write latency<br>âŒ Memory usage spikes during updates | Read-dominated workloads (99%+ reads), OS/systems programming |
| **Garbage Collection** | âœ… Fully automatic<br>âœ… No manual lifetime management<br>âœ… Handles cycles naturally<br>âœ… Simple programming model<br>âœ… No guards or special types | âŒ Not available in Rust<br>âŒ Unpredictable latency (GC pauses)<br>âŒ Memory overhead<br>âŒ No control over reclamation<br>âŒ Requires runtime support | Not applicable in Rust |
| **Locks** (RwLock/Mutex) | âœ… Simple and safe<br>âœ… Predictable behavior<br>âœ… Works with any data<br>âœ… Good tooling support<br>âœ… Easy to reason about | âŒ High contention overhead<br>âŒ Can cause deadlocks<br>âŒ Poor scalability<br>âŒ Reader-writer conflicts<br>âŒ Not lock-free | Simple cases, correctness over performance |

| If you need... | Choose... |
|----------------|-----------|
| ğŸš€ Maximum read performance | Guards/EBR |
| ğŸ¯ Simplicity | Arc or Locks |
| ğŸ“ Bounded memory | Hazard Pointers |
| âš¡ Async compatibility | Arc |
| ğŸ“– 99%+ read workload | RCU (if available) |
| ğŸ”§ Complex data structure internals | Guards/EBR |
| â³ Long blocking operations | Hazard Pointers or Arc |
| â±ï¸ Predictable latency | Hazard Pointers or Arc |

### Arc
```rust 
type Value = Arc<T>;
```